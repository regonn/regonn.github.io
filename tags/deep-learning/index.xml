<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on そこのけそこのけゆとりが通る</title>
    <link>https://blog.regonn.tokyo/tags/deep-learning/</link>
    <description>Recent content in deep-learning on そこのけそこのけゆとりが通る</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Fri, 27 Oct 2017 00:21:28 +0900</lastBuildDate>
    
	<atom:link href="https://blog.regonn.tokyo/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning 補足動画 行列内積の誤差逆伝播の式を導出</title>
      <link>https://blog.regonn.tokyo/data-science/2017-10-27-deep-learning/</link>
      <pubDate>Fri, 27 Oct 2017 00:21:28 +0900</pubDate>
      
      <guid>https://blog.regonn.tokyo/data-science/2017-10-27-deep-learning/</guid>
      <description>ゼロから作る Deep Learning で Affine レイヤーの内積の誤差逆伝播を求める部分が省略されていたので、行列で微分する部分の説明を動画で録画してみました。
Deep Learning 補足動画 行列での微分部分の補足動画です。
この
について解説してく。
今回想定するモデル 今回はバイアス部分は除いて、単純に入力 2 個と次の層への出力 3 個で考える。
ここで
となる。
① について まで計算しておいて
を代入すると
よって示せた。
② について 同じように
少し複雑になるが
なので、ほとんどは 0 になってしまう。
よって示せた。
単純な数値計算例(値は結構適当) もし、損失関数を計算して
として値が渡ってきたとしたら、</description>
    </item>
    
  </channel>
</rss>